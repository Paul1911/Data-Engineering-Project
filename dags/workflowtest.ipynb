{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helpfiles.temp_save_load import save_files, load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import helpfiles.ml_pipeline_config as configurations\n",
    "\n",
    "#Load data\n",
    "df = pd.read_csv(\"C:/Users/PC Paul/Downloads/DataEngOwnProject/data/creditcard.csv\")#.head(10000) \n",
    "while len(df.index) < 1000000:\n",
    "    df = pd.concat([df,df])\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#scale_data():\n",
    "rob_scaler = RobustScaler()\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#split_data_train_val():\n",
    "split_ratio = configurations.params['test_split_ratio']\n",
    "# Splitting\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n",
    "# Dataframe is concatenated again as the full dataset is needed and split again during cross validation. _val dataframes are needed for the final evaluation. \n",
    "df = pd.concat([X_train, y_train],axis = 1)\n",
    "\n",
    "#undersample_data():\n",
    "df2 = df.sample(frac=1) # this shuffles the initial df\n",
    "# Filter fraud cases and concat equal amount of non-fraud-cases\n",
    "fraud_df = df2.loc[df['Class'] == 1]\n",
    "non_fraud_df = df2.loc[df['Class'] == 0][:len(fraud_df)]\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.16992898e+00,  3.63965400e+00, -4.50849779e+00,\n",
       "         2.73066815e+00, -2.12269287e+00, -2.34101683e+00,\n",
       "        -4.23525308e+00,  1.70353766e+00, -1.30527913e+00,\n",
       "        -6.71672002e+00,  6.35361232e+00, -8.60164826e+00,\n",
       "         4.49930038e-01, -7.50616937e+00, -4.38081782e-01,\n",
       "        -3.69451600e+00, -6.30475339e+00, -1.26758713e+00,\n",
       "         3.57987030e-01,  5.00779054e-01,  6.45103276e-01,\n",
       "        -5.03529449e-01, -5.22821818e-04,  7.16957788e-02,\n",
       "         9.20074334e-02,  3.08497932e-01,  5.52590915e-01,\n",
       "         2.98954479e-01, -2.93419030e-01, -8.93773496e-01]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_observation = new_df.iloc[4:5, :-1].values\n",
    "test_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "svc.predict(test_observation)\n",
    "\n",
    "\n",
    "now = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "filename = 'model_' + '.pkl'\n",
    "joblib.dump(svc, filename, compress=1)\n",
    "mj = joblib.load(filename)\n",
    "mj.predict(test_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross Validation F1 score:  94.15%\n",
      "Best Parameters Logistic Regression: {'C': 10, 'penalty': 'l2'}\n",
      "Support Vector Classifier Cross Validation F1 score 93.99%\n",
      "Best Parameters SVM: {'C': 0.5, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "\n",
    "# Other Libraries\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "#from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.under_sampling import NearMiss\n",
    "#from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "\n",
    "import helpfiles.ml_pipeline_config as configurations\n",
    "\n",
    "#experiment():\n",
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']\n",
    "\n",
    "#classifiers = {\n",
    "#\"LogisticRegression\": LogisticRegression(),\n",
    "#\"Support Vector Classifier\": SVC()\n",
    "#}\n",
    "\n",
    "#for key, classifier in classifiers.items():\n",
    "#classifier.fit(X, y)\n",
    "#training_score = cross_val_score(classifier, X, y, cv=5, scoring = 'f1')\n",
    "#print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training F1 of\", round(training_score.mean(), 2) * 100)\n",
    "\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_maxiter = configurations.params[\"logreg_maxiter\"]\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} #todo: put this into pipeline config\n",
    "# Grid search\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, scoring = 'f1')\n",
    "#grid_log_reg = GridSearchCV(LogisticRegression(max_iter = log_reg_maxiter), log_reg_params) # todo: see whether you can use this line\n",
    "grid_log_reg.fit(X, y)\n",
    "# Logistic Regression estimator:\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']} #todo: put this into pipeline config\n",
    "# Grid search\n",
    "grid_svc = GridSearchCV(SVC(), svc_params, scoring = 'f1')\n",
    "grid_svc.fit(X, y)\n",
    "# SVC best estimator\n",
    "svc = grid_svc.best_estimator_\n",
    "\n",
    "# Evaluation\n",
    "# Logistic Regression \n",
    "log_reg_score = cross_val_score(log_reg, X, y, cv=5, scoring = 'f1')\n",
    "print('Logistic Regression Cross Validation F1 score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Best Parameters Logistic Regression: \" + str(grid_log_reg.best_params_))\n",
    "# SVC\n",
    "svc_score = cross_val_score(svc, X, y, cv=5, scoring = 'f1')\n",
    "print('Support Vector Classifier Cross Validation F1 score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Best Parameters SVM: \" + str(grid_svc.best_params_))\n",
    "\n",
    "#X_val = load_files(['X_val'])[0]\n",
    "#y_val = load_files(['y_val'])[0]\n",
    "\n",
    "#Predict Y\n",
    "y_pred_log_reg = log_reg.predict(X_val)\n",
    "y_pred_SVM = svc.predict(X_val)\n",
    "f1_log_reg = f1_score(y_val, y_pred_log_reg)\n",
    "f1_SVM = f1_score(y_val, y_pred_SVM)\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "log_reg_results = pd.DataFrame([[\n",
    "    now,\n",
    "    'Logistic Regression',\n",
    "    grid_log_reg.best_estimator_,\n",
    "    grid_log_reg.best_params_,\n",
    "    f1_log_reg\n",
    "    ]],\n",
    "    columns = [\n",
    "        'experiment_date',\n",
    "        'method',\n",
    "        'best_estimator',\n",
    "        'best_parameters',\n",
    "        'f1_score'])\n",
    "\n",
    "svc_results = pd.DataFrame([[\n",
    "    now,\n",
    "    'Support Vector Classifier',\n",
    "    grid_svc.best_estimator_,\n",
    "    grid_svc.best_params_,\n",
    "    f1_SVM\n",
    "    ]],\n",
    "    columns = [\n",
    "        'experiment_date',\n",
    "        'method',\n",
    "        'best_estimator',\n",
    "        'best_parameters',\n",
    "        'f1_score'])\n",
    "\n",
    "results = pd.concat([log_reg_results,svc_results])\n",
    "\n",
    "#Creation of the dataset of evalutation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_date</th>\n",
       "      <th>method</th>\n",
       "      <th>best_estimator</th>\n",
       "      <th>best_parameters</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-10_11-22-02</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>LogisticRegression(C=10)</td>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>0.122163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-10_11-22-02</td>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>SVC(C=0.5, kernel='linear')</td>\n",
       "      <td>{'C': 0.5, 'kernel': 'linear'}</td>\n",
       "      <td>0.150354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       experiment_date                     method  \\\n",
       "0  2022-03-10_11-22-02        Logistic Regression   \n",
       "0  2022-03-10_11-22-02  Support Vector Classifier   \n",
       "\n",
       "                best_estimator                 best_parameters  f1_score  \n",
       "0     LogisticRegression(C=10)      {'C': 10, 'penalty': 'l2'}  0.122163  \n",
       "0  SVC(C=0.5, kernel='linear')  {'C': 0.5, 'kernel': 'linear'}  0.150354  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second try as a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialze the estimators\n",
    "clf1 = RandomForestClassifier(random_state=42)\n",
    "clf2 = SVC(probability=True, random_state=42)\n",
    "clf3 = LogisticRegression(random_state=42)\n",
    "clf4 = DecisionTreeClassifier(random_state=42)\n",
    "clf5 = KNeighborsClassifier()\n",
    "clf6 = MultinomialNB()\n",
    "clf7 = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Initiaze the hyperparameters for each dictionary\n",
    "param1 = {}\n",
    "param1['classifier__n_estimators'] = [10, 50, 100, 250]\n",
    "param1['classifier__max_depth'] = [5, 10, 20]\n",
    "param1['classifier__class_weight'] = [None, {0:1,1:5}, {0:1,1:10}, {0:1,1:25}]\n",
    "param1['classifier'] = [clf1]\n",
    "\n",
    "param2 = {}\n",
    "param2['classifier__C'] = [10**-2, 10**-1, 10**0, 10**1, 10**2]\n",
    "param2['classifier__class_weight'] = [None, {0:1,1:5}, {0:1,1:10}, {0:1,1:25}]\n",
    "param2['classifier'] = [clf2]\n",
    "\n",
    "param3 = {}\n",
    "param3['classifier__C'] = [10**-2, 10**-1, 10**0, 10**1, 10**2]\n",
    "param3['classifier__penalty'] = ['l1', 'l2']\n",
    "param3['classifier__class_weight'] = [None, {0:1,1:5}, {0:1,1:10}, {0:1,1:25}]\n",
    "param3['classifier'] = [clf3]\n",
    "\n",
    "param4 = {}\n",
    "param4['classifier__max_depth'] = [5,10,25,None]\n",
    "param4['classifier__min_samples_split'] = [2,5,10]\n",
    "param4['classifier__class_weight'] = [None, {0:1,1:5}, {0:1,1:10}, {0:1,1:25}]\n",
    "param4['classifier'] = [clf4]\n",
    "\n",
    "param5 = {}\n",
    "param5['classifier__n_neighbors'] = [2,5,10,25,50]\n",
    "param5['classifier'] = [clf5]\n",
    "\n",
    "param6 = {}\n",
    "param6['classifier__alpha'] = [10**0, 10**1, 10**2]\n",
    "param6['classifier'] = [clf6]\n",
    "\n",
    "param7 = {}\n",
    "param7['classifier__n_estimators'] = [10, 50, 100, 250]\n",
    "param7['classifier__max_depth'] = [5, 10, 20]\n",
    "param7['classifier'] = [clf7]\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('classifier', clf1)])\n",
    "params = [param1, param2, param3, param4, param5, param6, param7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.2 s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%%time\n",
    "# Train the grid search model\n",
    "gs = GridSearchCV(pipeline, params, cv=3, n_jobs=-1, scoring='roc_auc').fit(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': GradientBoostingClassifier(max_depth=5, random_state=42),\n",
       " 'classifier__max_depth': 5,\n",
       " 'classifier__n_estimators': 100}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9900020081692865"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_f1 = cross_val_score(gs.best_estimator_, X, y, cv=5, scoring = 'f1')\n",
    "score_f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22436363636363638"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict Y\n",
    "y_pred= gs.best_estimator_.predict(X_val)\n",
    "f1_val = f1_score(y_val, y_pred)\n",
    "f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bc10dd72c8aaf2d861c41511aea097bfd1dfda640e26e44598e27f4cfd3593d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
