{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helpfiles.temp_save_load import save_files, load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import helpfiles.ml_pipeline_config as configurations\n",
    "\n",
    "#Load data\n",
    "df = pd.read_csv(\"C:/Users/PC Paul/Downloads/DataEngOwnProject/data/creditcard.csv\")#.head(10000) \n",
    "while len(df.index) < 1000000: # Todo: uncomment\n",
    "    df = pd.concat([df,df])\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#scale_data():\n",
    "rob_scaler = RobustScaler()\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#split_data_train_val():\n",
    "split_ratio = configurations.params['test_split_ratio']\n",
    "# Splitting\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "# Dataframe is concatenated again as the full dataset is needed and split again during cross validation. _val dataframes are needed for the final evaluation. \n",
    "df = pd.concat([X_train, y_train],axis = 1)\n",
    "\n",
    "#undersample_data():\n",
    "df2 = df.sample(frac=1) # this shuffles the initial df\n",
    "# Filter fraud cases and concat equal amount of non-fraud-cases\n",
    "fraud_df = df2.loc[df['Class'] == 1]\n",
    "non_fraud_df = df2.loc[df['Class'] == 0][:len(fraud_df)]\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355396</th>\n",
       "      <td>-2.042608</td>\n",
       "      <td>1.573578</td>\n",
       "      <td>-2.372652</td>\n",
       "      <td>-0.572676</td>\n",
       "      <td>-2.097353</td>\n",
       "      <td>-0.174142</td>\n",
       "      <td>-3.039520</td>\n",
       "      <td>-1.634233</td>\n",
       "      <td>-0.594809</td>\n",
       "      <td>-5.459602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501222</td>\n",
       "      <td>-0.696892</td>\n",
       "      <td>-0.600514</td>\n",
       "      <td>0.127547</td>\n",
       "      <td>-0.786072</td>\n",
       "      <td>0.606097</td>\n",
       "      <td>0.171697</td>\n",
       "      <td>3.351544</td>\n",
       "      <td>-0.361313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681343</th>\n",
       "      <td>-0.508574</td>\n",
       "      <td>0.326057</td>\n",
       "      <td>2.258090</td>\n",
       "      <td>0.085603</td>\n",
       "      <td>-1.266965</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>0.147233</td>\n",
       "      <td>0.069141</td>\n",
       "      <td>0.659956</td>\n",
       "      <td>-0.635383</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091377</td>\n",
       "      <td>0.057221</td>\n",
       "      <td>0.784309</td>\n",
       "      <td>-0.689903</td>\n",
       "      <td>0.940863</td>\n",
       "      <td>-0.013294</td>\n",
       "      <td>0.124774</td>\n",
       "      <td>0.661590</td>\n",
       "      <td>-0.145078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735224</th>\n",
       "      <td>1.956552</td>\n",
       "      <td>-0.058671</td>\n",
       "      <td>-1.036132</td>\n",
       "      <td>1.291854</td>\n",
       "      <td>0.176632</td>\n",
       "      <td>-0.164698</td>\n",
       "      <td>-0.000702</td>\n",
       "      <td>-0.058953</td>\n",
       "      <td>0.466270</td>\n",
       "      <td>0.320406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706556</td>\n",
       "      <td>-0.039530</td>\n",
       "      <td>-0.374002</td>\n",
       "      <td>0.331067</td>\n",
       "      <td>-0.456771</td>\n",
       "      <td>0.022007</td>\n",
       "      <td>-0.063080</td>\n",
       "      <td>-0.167808</td>\n",
       "      <td>0.385867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758678</th>\n",
       "      <td>1.546322</td>\n",
       "      <td>-0.754504</td>\n",
       "      <td>-2.370798</td>\n",
       "      <td>0.106949</td>\n",
       "      <td>1.042846</td>\n",
       "      <td>1.300035</td>\n",
       "      <td>-0.062101</td>\n",
       "      <td>0.397464</td>\n",
       "      <td>0.841893</td>\n",
       "      <td>-0.937829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335966</td>\n",
       "      <td>0.047470</td>\n",
       "      <td>-1.077996</td>\n",
       "      <td>-0.283541</td>\n",
       "      <td>-0.025376</td>\n",
       "      <td>-0.003459</td>\n",
       "      <td>-0.007373</td>\n",
       "      <td>2.444600</td>\n",
       "      <td>0.511713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407288</th>\n",
       "      <td>-0.500881</td>\n",
       "      <td>0.268217</td>\n",
       "      <td>1.981096</td>\n",
       "      <td>-1.316633</td>\n",
       "      <td>-0.762119</td>\n",
       "      <td>-0.462187</td>\n",
       "      <td>-0.093959</td>\n",
       "      <td>0.161073</td>\n",
       "      <td>1.460445</td>\n",
       "      <td>-1.391160</td>\n",
       "      <td>...</td>\n",
       "      <td>1.112432</td>\n",
       "      <td>-0.156227</td>\n",
       "      <td>0.424759</td>\n",
       "      <td>-0.399876</td>\n",
       "      <td>-0.639136</td>\n",
       "      <td>0.116901</td>\n",
       "      <td>0.164423</td>\n",
       "      <td>-0.292022</td>\n",
       "      <td>-0.095348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844089</th>\n",
       "      <td>-0.956390</td>\n",
       "      <td>2.361594</td>\n",
       "      <td>-3.171195</td>\n",
       "      <td>1.970759</td>\n",
       "      <td>0.474761</td>\n",
       "      <td>-1.902598</td>\n",
       "      <td>-0.055178</td>\n",
       "      <td>0.277831</td>\n",
       "      <td>-1.745854</td>\n",
       "      <td>-2.516628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719400</td>\n",
       "      <td>0.122458</td>\n",
       "      <td>-0.255650</td>\n",
       "      <td>-0.619259</td>\n",
       "      <td>-0.484280</td>\n",
       "      <td>0.683535</td>\n",
       "      <td>0.443299</td>\n",
       "      <td>0.250105</td>\n",
       "      <td>0.955545</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005108</th>\n",
       "      <td>-10.300820</td>\n",
       "      <td>6.483095</td>\n",
       "      <td>-15.076363</td>\n",
       "      <td>6.554191</td>\n",
       "      <td>-8.880252</td>\n",
       "      <td>-4.471672</td>\n",
       "      <td>-14.900689</td>\n",
       "      <td>3.840170</td>\n",
       "      <td>-4.358441</td>\n",
       "      <td>-14.533162</td>\n",
       "      <td>...</td>\n",
       "      <td>1.041642</td>\n",
       "      <td>-0.682790</td>\n",
       "      <td>0.573544</td>\n",
       "      <td>-1.602389</td>\n",
       "      <td>-0.393521</td>\n",
       "      <td>-0.468893</td>\n",
       "      <td>0.105920</td>\n",
       "      <td>-0.293419</td>\n",
       "      <td>0.108141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724332</th>\n",
       "      <td>-5.603690</td>\n",
       "      <td>5.222193</td>\n",
       "      <td>-7.516830</td>\n",
       "      <td>8.117724</td>\n",
       "      <td>-2.756858</td>\n",
       "      <td>-1.574565</td>\n",
       "      <td>-6.330343</td>\n",
       "      <td>2.998419</td>\n",
       "      <td>-4.508167</td>\n",
       "      <td>-7.334377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428408</td>\n",
       "      <td>-0.101184</td>\n",
       "      <td>-0.520199</td>\n",
       "      <td>-0.176938</td>\n",
       "      <td>0.461450</td>\n",
       "      <td>-0.106625</td>\n",
       "      <td>-0.479662</td>\n",
       "      <td>-0.307391</td>\n",
       "      <td>0.211196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327356</th>\n",
       "      <td>-5.314173</td>\n",
       "      <td>4.145944</td>\n",
       "      <td>-8.532522</td>\n",
       "      <td>8.344392</td>\n",
       "      <td>-5.718008</td>\n",
       "      <td>-3.043536</td>\n",
       "      <td>-10.989185</td>\n",
       "      <td>3.404129</td>\n",
       "      <td>-6.167234</td>\n",
       "      <td>-11.435624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862996</td>\n",
       "      <td>-0.614453</td>\n",
       "      <td>0.523648</td>\n",
       "      <td>-0.712593</td>\n",
       "      <td>0.324638</td>\n",
       "      <td>2.245091</td>\n",
       "      <td>0.497321</td>\n",
       "      <td>0.925388</td>\n",
       "      <td>-0.511572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401571</th>\n",
       "      <td>1.143678</td>\n",
       "      <td>0.208608</td>\n",
       "      <td>0.130054</td>\n",
       "      <td>0.582732</td>\n",
       "      <td>-0.159519</td>\n",
       "      <td>-0.947460</td>\n",
       "      <td>0.430275</td>\n",
       "      <td>-0.238180</td>\n",
       "      <td>-0.279506</td>\n",
       "      <td>-0.129685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954394</td>\n",
       "      <td>0.178521</td>\n",
       "      <td>0.413463</td>\n",
       "      <td>0.169508</td>\n",
       "      <td>0.142055</td>\n",
       "      <td>-0.051653</td>\n",
       "      <td>0.017001</td>\n",
       "      <td>0.319966</td>\n",
       "      <td>-0.120688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3204 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                V1        V2         V3        V4        V5        V6  \\\n",
       "355396   -2.042608  1.573578  -2.372652 -0.572676 -2.097353 -0.174142   \n",
       "681343   -0.508574  0.326057   2.258090  0.085603 -1.266965 -0.218649   \n",
       "735224    1.956552 -0.058671  -1.036132  1.291854  0.176632 -0.164698   \n",
       "758678    1.546322 -0.754504  -2.370798  0.106949  1.042846  1.300035   \n",
       "407288   -0.500881  0.268217   1.981096 -1.316633 -0.762119 -0.462187   \n",
       "...            ...       ...        ...       ...       ...       ...   \n",
       "844089   -0.956390  2.361594  -3.171195  1.970759  0.474761 -1.902598   \n",
       "1005108 -10.300820  6.483095 -15.076363  6.554191 -8.880252 -4.471672   \n",
       "724332   -5.603690  5.222193  -7.516830  8.117724 -2.756858 -1.574565   \n",
       "327356   -5.314173  4.145944  -8.532522  8.344392 -5.718008 -3.043536   \n",
       "401571    1.143678  0.208608   0.130054  0.582732 -0.159519 -0.947460   \n",
       "\n",
       "                V7        V8        V9        V10  ...       V22       V23  \\\n",
       "355396   -3.039520 -1.634233 -0.594809  -5.459602  ...  0.501222 -0.696892   \n",
       "681343    0.147233  0.069141  0.659956  -0.635383  ... -0.091377  0.057221   \n",
       "735224   -0.000702 -0.058953  0.466270   0.320406  ...  0.706556 -0.039530   \n",
       "758678   -0.062101  0.397464  0.841893  -0.937829  ... -0.335966  0.047470   \n",
       "407288   -0.093959  0.161073  1.460445  -1.391160  ...  1.112432 -0.156227   \n",
       "...            ...       ...       ...        ...  ...       ...       ...   \n",
       "844089   -0.055178  0.277831 -1.745854  -2.516628  ...  0.719400  0.122458   \n",
       "1005108 -14.900689  3.840170 -4.358441 -14.533162  ...  1.041642 -0.682790   \n",
       "724332   -6.330343  2.998419 -4.508167  -7.334377  ...  0.428408 -0.101184   \n",
       "327356  -10.989185  3.404129 -6.167234 -11.435624  ...  0.862996 -0.614453   \n",
       "401571    0.430275 -0.238180 -0.279506  -0.129685  ... -0.954394  0.178521   \n",
       "\n",
       "              V24       V25       V26       V27       V28  scaled_amount  \\\n",
       "355396  -0.600514  0.127547 -0.786072  0.606097  0.171697       3.351544   \n",
       "681343   0.784309 -0.689903  0.940863 -0.013294  0.124774       0.661590   \n",
       "735224  -0.374002  0.331067 -0.456771  0.022007 -0.063080      -0.167808   \n",
       "758678  -1.077996 -0.283541 -0.025376 -0.003459 -0.007373       2.444600   \n",
       "407288   0.424759 -0.399876 -0.639136  0.116901  0.164423      -0.292022   \n",
       "...           ...       ...       ...       ...       ...            ...   \n",
       "844089  -0.255650 -0.619259 -0.484280  0.683535  0.443299       0.250105   \n",
       "1005108  0.573544 -1.602389 -0.393521 -0.468893  0.105920      -0.293419   \n",
       "724332  -0.520199 -0.176938  0.461450 -0.106625 -0.479662      -0.307391   \n",
       "327356   0.523648 -0.712593  0.324638  2.245091  0.497321       0.925388   \n",
       "401571   0.413463  0.169508  0.142055 -0.051653  0.017001       0.319966   \n",
       "\n",
       "         scaled_time  Class  \n",
       "355396     -0.361313      1  \n",
       "681343     -0.145078      0  \n",
       "735224      0.385867      0  \n",
       "758678      0.511713      0  \n",
       "407288     -0.095348      0  \n",
       "...              ...    ...  \n",
       "844089      0.955545      1  \n",
       "1005108     0.108141      1  \n",
       "724332      0.211196      1  \n",
       "327356     -0.511572      1  \n",
       "401571     -0.120688      0  \n",
       "\n",
       "[3204 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross Validation F1 score:  94.48%\n",
      "Best Parameters Logistic Regression: {'C': 100, 'penalty': 'l2'}\n",
      "Support Vector Classifier Cross Validation F1 score 94.63%\n",
      "Best Parameters SVM: {'C': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "\n",
    "# Other Libraries\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "#from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.under_sampling import NearMiss\n",
    "#from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "\n",
    "import helpfiles.ml_pipeline_config as configurations\n",
    "\n",
    "#experiment():\n",
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']\n",
    "\n",
    "#classifiers = {\n",
    "#\"LogisticRegression\": LogisticRegression(),\n",
    "#\"Support Vector Classifier\": SVC()\n",
    "#}\n",
    "\n",
    "#for key, classifier in classifiers.items():\n",
    "#classifier.fit(X, y)\n",
    "#training_score = cross_val_score(classifier, X, y, cv=5, scoring = 'f1')\n",
    "#print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training F1 of\", round(training_score.mean(), 2) * 100)\n",
    "\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_maxiter = configurations.params[\"logreg_maxiter\"]\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} #todo: put this into pipeline config\n",
    "# Grid search\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "#grid_log_reg = GridSearchCV(LogisticRegression(max_iter = log_reg_maxiter), log_reg_params) # todo: see whether you can use this line\n",
    "grid_log_reg.fit(X, y)\n",
    "# Logistic Regression estimator:\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']} #todo: put this into pipeline config\n",
    "# Grid search\n",
    "grid_svc = GridSearchCV(SVC(), svc_params)\n",
    "grid_svc.fit(X, y)\n",
    "# SVC best estimator\n",
    "svc = grid_svc.best_estimator_\n",
    "\n",
    "# Evaluation\n",
    "# Logistic Regression \n",
    "log_reg_score = cross_val_score(log_reg, X, y, cv=5, scoring = 'f1')\n",
    "print('Logistic Regression Cross Validation F1 score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Best Parameters Logistic Regression: \" + str(grid_log_reg.best_params_))\n",
    "# SVC\n",
    "svc_score = cross_val_score(svc, X, y, cv=5, scoring = 'f1')\n",
    "print('Support Vector Classifier Cross Validation F1 score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Best Parameters SVM: \" + str(grid_svc.best_params_))\n",
    "\n",
    "#X_val = load_files(['X_val'])[0]\n",
    "#y_val = load_files(['y_val'])[0]\n",
    "\n",
    "#Predict Y\n",
    "y_pred_log_reg = log_reg.predict(X_val)\n",
    "y_pred_SVM = svc.predict(X_val)\n",
    "f1_log_reg = f1_score(y_val, y_pred_log_reg)\n",
    "f1_SVM = f1_score(y_val, y_pred_SVM)\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "log_reg_results = pd.DataFrame([[\n",
    "    now,\n",
    "    'Logistic Regression',\n",
    "    grid_log_reg.best_estimator_,\n",
    "    grid_log_reg.best_params_,\n",
    "    f1_log_reg\n",
    "    ]],\n",
    "    columns = [\n",
    "        'experiment_date',\n",
    "        'method',\n",
    "        'best_estimator',\n",
    "        'best_parameters',\n",
    "        'f1_score'])\n",
    "log_reg_results\n",
    "\n",
    "svc_results = pd.DataFrame([[\n",
    "    now,\n",
    "    'Support Vector Classifier',\n",
    "    grid_svc.best_estimator_,\n",
    "    grid_svc.best_params_,\n",
    "    f1_SVM\n",
    "    ]],\n",
    "    columns = [\n",
    "        'experiment_date',\n",
    "        'method',\n",
    "        'best_estimator',\n",
    "        'best_parameters',\n",
    "        'f1_score'])\n",
    "\n",
    "results = pd.concat([log_reg_results,svc_results])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bc10dd72c8aaf2d861c41511aea097bfd1dfda640e26e44598e27f4cfd3593d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
